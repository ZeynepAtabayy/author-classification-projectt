{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seab as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# === BERT MODEL & TOKENIZER ===\n",
    "model_name = \"dbmdz/bert-base-turkish-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# === CLEAN TEXT FUNCTION ===\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-ZçÇğĞıİöÖşŞüÜ ]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = text.replace(\"i̇\", \"i\")\n",
    "    return text\n",
    "\n",
    "# === MEAN POOLING FUNCTION ===\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return (token_embeddings * input_mask_expanded).sum(1) / input_mask_expanded.sum(1)\n",
    "\n",
    "# === LOAD DATA ===\n",
    "texts = []\n",
    "labels = []\n",
    "main_folder = r\"C:\\Users\\YUCE037\\Downloads\\AAydintasbas\"\n",
    "\n",
    "for author_folder in os.listdir(main_folder):\n",
    "    author_path = os.path.join(main_folder, author_folder)\n",
    "    if os.path.isdir(author_path):\n",
    "        for txt_file in os.listdir(author_path):\n",
    "            file_path = os.path.join(author_path, txt_file)\n",
    "            if file_path.endswith(\".txt\"):\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    texts.append(f.read())\n",
    "                    labels.append(author_folder)\n",
    "\n",
    "df = pd.DataFrame({\"text\": texts, \"author\": labels})\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a1c630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === EXTRACT EMBEDDINGS ===\n",
    "embeddings = []\n",
    "batch_size = 8\n",
    "for i in tqdm(range(0, len(df), batch_size)):\n",
    "    batch_texts = df[\"clean_text\"].iloc[i:i+batch_size].tolist()\n",
    "    encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    batch_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "X_bert = torch.cat(embeddings).cpu().numpy()\n",
    "y = df[\"author\"].values\n",
    "\n",
    "# === LABEL ENCODE ===\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# === SPLIT ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bert, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd01b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred = model_rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model_svm = SVC(kernel='linear', probability=True)\n",
    "model_svm.fit(X_train, y_train)\n",
    "y_pred = model_svm.predict(X_test)\n",
    "\n",
    "print(\"SVM (Linear) Results:\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model_nb = GaussianNB()\n",
    "model_nb.fit(X_train, y_train)\n",
    "y_pred = model_nb.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "model_mlp.fit(X_train, y_train)\n",
    "y_pred = model_mlp.predict(X_test)\n",
    "\n",
    "print(\"MLP Results:\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_tree = DecisionTreeClassifier(random_state=42)\n",
    "model_tree.fit(X_train, y_train)\n",
    "y_pred = model_tree.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Results:\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fbd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred = model_xgb.predict(X_test)\n",
    "\n",
    "print(\"XGBoost Results:\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === OVERFITTING CONTROL FOR DECISION TREE ===\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_preds = model_tree.predict(X_train)\n",
    "test_preds = model_tree.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "\n",
    "print(\"Overfitting Evaluation (Decision Tree):\")\n",
    "print(f\"Training Accuracy: {train_acc:.2%}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
